{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Materi Meetup DSI Jateng 4** | *24 February 2019 | DSI Jateng*\n",
    "# Introduction to Sentiment Analysis\n",
    "\n",
    "***\n",
    "Sentiment Analysis adalah proses melakukan identifikasi dan ekstrak informasi dari data teks untuk mengetahui sentimen dari produk yang bisa berupa positif, negatif, atau netral. Berikut ini adalah contohnya dari review film **Spider-Man: Into the Spider-Verse**:\n",
    "* positif: \"Keren banget animasinya, film animasi terbaik yang pernah saya tonton!\"\n",
    "* negatif: \"Biasa aja gak ada seru-serunya, alur ceritanya jelek.\"\n",
    "* netral: \"Pertamax gan!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings\n",
    "data = 'Helo dunia!'\n",
    "print(data)\n",
    "print(type(data))\n",
    "\n",
    "# TODO: cetak huruf 'H' pada kata 'helo'\n",
    "print(data)\n",
    "\n",
    "# TODO: cetak panjang teks\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers\n",
    "value = 10.01\n",
    "print(value, type(value))\n",
    "\n",
    "# TODO: ganti nilai pada value menjadi 10\n",
    "value = None\n",
    "print(value, type(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean\n",
    "a = True\n",
    "b = False\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = 1, 2, 3\n",
    "print(a, b, c)\n",
    "\n",
    "# TODO: ganti value b menjadi a, value c menjadi b, value a menjadi c\n",
    "a, b, c = None, None, None\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If-Then-Else Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 75\n",
    "\n",
    "# TODO: Buat sebuah if-else-then conditional\n",
    "# Jika, value 0-50 => Nilai C, \n",
    "#       value 51-75 => Nilai B,\n",
    "#       value >75 => Nilai A\n",
    "\n",
    "if value:\n",
    "    print('Nilainya C')\n",
    "elif value:\n",
    "    print ('Nilainya B')\n",
    "else:\n",
    "    print ('Nilainya A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For-Loop\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "# TODO: cetak angka dari 10 sampai 1 dengan perulangan for..in range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While-Loop\n",
    "i = 0\n",
    "while i < 10:\n",
    "    print (i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1, 2, 3)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3]\n",
    "print(\"Indeks ke-0: %d\" % mylist[0]) \n",
    "mylist.append(4)\n",
    "print(\"Panjang list: %d\" % len(mylist)) \n",
    "\n",
    "for value in mylist:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'a': 1, 'b': 2, 'c': 3}\n",
    "print(\"A nilainya: %d\" % mydict['a'])\n",
    "\n",
    "mydict['a'] = 11\n",
    "print(\"A nilainya: %d\" % mydict['a']) \n",
    "\n",
    "print(\"Keys: %s\" % mydict.keys()) \n",
    "print(\"Values: %s\" % mydict.values())\n",
    "\n",
    "for key in mydict.keys():\n",
    "    print (mydict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum function\n",
    "def tambah(x, y):\n",
    "    return x + y\n",
    "# Test tambah function\n",
    "result = tambah(1, 3)\n",
    "print(result)\n",
    "\n",
    "# TODO: Buat fungsi luas persegi panjang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hai pagi kamu.. Udah mulai rindu aku belum? Atau masih dalam buaian semu itu?\"\n",
    "\n",
    "# TODO: cetak panjang teks \n",
    "# Hasil = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cetak sebuah list kata-kata dari text2, dipisahkan oleh ' '\n",
    "text2 = None\n",
    "#len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension\n",
    "# TODO: cetak kata yang lebih dari 3 huruf/karakter dari text2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cetak kata yang dimulai huruf kapital dengan .istitle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cetak kata yang diakhiri huruf 'u' dengan .endswith()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words using set()\n",
    "text3 = 'To be or not to be'\n",
    "text4 = text3.split(' ')\n",
    "\n",
    "len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cetak kata unik dengan set\n",
    "len(set(text4))\n",
    "set(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: bagaimana keluaran jika huruf 'T' pada 'To' diperkecil menggunakan set\n",
    "len(set([w.lower() for w in text4])) # .lower converts the string to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([w.lower() for w in text4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing free-text\n",
    "text5 = 'Saw $25 @reliancejio phone with @google assistant, I thought Siri worked much better... disappointed'\n",
    "text6 = text5.split(' ')\n",
    "\n",
    "text6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: mencari kata yang terdapat @ dengan startswith()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # import re - a module that provides support for regular expressions\n",
    "\n",
    "# TODO:  mencari kata yang terdapat @ dengan regex\n",
    "[w for w in text6 if re.search('@[A-Za-z0-9_]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Sentiment Analysis\n",
    "## Step 1: Exploring the Data!\n",
    "\n",
    "Dataset yang digunakan adalah [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/). Dataset tersebut berisi review film dari website [imdb.com](imdb.com), yang setiap datanya sudah terdapat label 'positive' atau 'negative'.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011.\n",
    "\n",
    "Dataset sudah tersedia. Lakukan load data dengan code Python di bawah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/imdb-reviews'):\n",
    "    \"\"\"Read IMDb movie reviews from given directory.\n",
    "    \n",
    "    Directory structure expected:\n",
    "    - data/imdb-reviews\n",
    "        - train/\n",
    "            - pos/\n",
    "            - neg/\n",
    "        - test/\n",
    "            - pos/\n",
    "            - neg/\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Data, labels to be returned in nested dicts matching the dir. structure\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    # Assume 2 sub-directories: train, test\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Assume 2 sub-directories for sentiment (label): pos, neg\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # Fetch list of files for this sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # Read reviews data and assign labels\n",
    "            for f in files:\n",
    "                with open(f, 'rb') as review:\n",
    "                    data[data_type][sentiment].append(review.read().decode('utf-8'))\n",
    "                    labels[data_type][sentiment].append(sentiment)\n",
    "            \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "    \n",
    "    # Return data, labels as nested dicts\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jalankan fungsi read imdb data\n",
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos | {} neg, test = {} pos | {} neg\".format(\n",
    "        len(data['train']['pos']), len(data['train']['neg']),\n",
    "        len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba kita lihat positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train']['pos'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dan negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train']['neg'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasikan dengan WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing wordcloud\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "sentiment = 'pos'\n",
    "\n",
    "# Combine all reviews for the desired sentiment\n",
    "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "# Initialize wordcloud object\n",
    "wc = WordCloud(background_color='white', max_words=50,\n",
    "        # update stopwords to include common words like film and movie\n",
    "        stopwords = STOPWORDS.update(['br','film','movie']))\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba rubah sentiment menjadi 'neg' dan lihat perbedaannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = 'neg'\n",
    "\n",
    "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "wc = WordCloud(background_color = 'white', max_words=50, stopwords = STOPWORDS.update(['br', 'film', 'movie']))\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Form Training and test sets\n",
    "\n",
    "Setelah kita melihat bentuk data review seperti apa, selanjutnya bentuklah training set dan test set dari dokumen review positive dan negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    # TODO: Combine positive and negative reviews and labels\n",
    "    data_train = None\n",
    "    labels_train = None\n",
    "    \n",
    "    data_test = None\n",
    "    labels_test = None\n",
    "    \n",
    "    # TODO: Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = None\n",
    "    data_test, labels_test = None\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jalankan fungsi prepare_imdb_data\n",
    "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing\n",
    "\n",
    "Jika kita melihat contoh review, data yang kita miliki masih terdapat tag HTML, oleh karena itu perlu <font color=blue>melakukan remove tag HTML</font>. Kita juga perlu <font color=blue>menghapus karakter bukan huruf, melakukan normalisasi huruf dari uppercase ke lowercase, melakukan tokenisasi, menghapus stop words, dan melakukan stemming pada setiap kata pada dokumen.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Convert each review to words\n",
    "\n",
    "Tugas selanjutnya lengkapi fungsi <code>review_to_words()</code> yang menjalankan semua proses *preprocessing*. Pastikan sudah berhasil dilakukan *import* untuk *library* yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup to easily remove HTML tags\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "    \n",
    "    # TODO: Remove HTML tags and non-letters,\n",
    "    soup = BeautifulSoup(review, 'html5lib')\n",
    "    text = None\n",
    "    \n",
    "    # TODO: convert to lowercase, tokenize\n",
    "    text = None\n",
    "    words = None\n",
    "    \n",
    "    # TODO: remove stopwords and stem\n",
    "    words = None\n",
    "    words = None\n",
    "\n",
    "    # Return final list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jalankan fungsi review_to_words\n",
    "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
    "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ketika fungsi <code>review_to_words()</code> sudah disempurnakan, kita bisa menerapkannya pada data training dan data test. Prosesnya butuh waktu yang cukup lama, jadi kita membuat *cache file* agar bisa di*retrieve* nanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(review_to_words, data_train))\n",
    "        words_test = list(map(review_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting Bag-of-Words features\n",
    "\n",
    "Setelah document review telah dilakukan *preprocessing*, kita bisa melakukan transformasi menjadi representasi fitur Bag-of-Words. Kita hanya akan melakukannya hanya pada data training, karena kita tidak boleh melihat data testing sama sekali!\n",
    "\n",
    "**Vocabulary V** (kumpulan kata unik dokumen dari data training) akan digunakan saat melakukan training dengan algoritma *supervised learning*. Setiap data test harus dilakukan transform dengan cara yang sama untuk melakukan prediksi. Jadi, mengingat pentingnya data hasil *transform* dan *vocabulary*, maka akan kita simpan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Compute Bag-of-Words features\n",
    "\n",
    "Lengkapi fungsi <code>extract_BoW_features()</code>, kemudian terapkan pada data training dan data testing, dan simpan hasilnya pada variable <code>features_train</code> dan <code>features_test</code>. Tentukan ukuran vocabulary, misalkan $|V| = 5000$, maka yang digunakan adalah top $|V|$ dan sisanya diabaikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # TODO: Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = None\n",
    "        features_train = None\n",
    "\n",
    "        # TODO: Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = None\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)\n",
    "\n",
    "# Inspect the vocabulary that was computed\n",
    "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
    "\n",
    "import random\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "\n",
    "# Sample\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[5])\n",
    "print(\"\\n--- Bag-of-Words features ---\")\n",
    "print(features_train[5])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan visualisasi fitur Bag-of-Words pada salah satu document training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the BoW feature vector for a training document\n",
    "plt.plot(features_train[5,:])\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Normalize feature vectors\n",
    "\n",
    "Fitur Bag-of-Words berisi jumlah pada masing-masing kata. Tetapi, jumlah pada masing-masing kata bisa bervariasi, dan memiliki potensi menurunkan performa *learning algorithm*. Jadi, kita lakukan normalisasi fitur BoW agar memiliki satuan panjang.\n",
    "\n",
    "Hal ini untuk menjaga representasi dari dokumen dan juga mencegah dokumen yang memiliki jumlah kata besar mendominasi kata  yang berjumlah sedikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# TODO: Normalize BoW features in training and test set\n",
    "features_train = None\n",
    "features_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification using BoW features\n",
    "\n",
    "Setelah data dilakukan transformasi, kita bisa memasukkan ke dalam *classifier*. Kita menggunakan Naive Bayes classifier dari library scikit-learn, dan kemudian kita lakukan evaluasi pada data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: Train a Gaussian Naive Bayes classifier\n",
    "clf = None\n",
    "clf.fit(None)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf.__class__.__name__,\n",
    "        clf.score(features_train, labels_train),\n",
    "        clf.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Testing/Pengujian\n",
    "\n",
    "Buatlah sebuah review movie pendek, kemudian coba keluarkan hasilnya, apakah positif atau negatif?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a sample review and set its true sentiment\n",
    "my_review = \"Despite a compelling lead performance by Tom Hanks and a great soundtrack, Forrest Gump never gets out of the shadow of its weak plot and questionable premise\"\n",
    "true_sentiment = 'neg'  # sentiment must be 'pos' or 'neg'\n",
    "\n",
    "# TODO: Apply the same preprocessing and vectorizing steps as you did for your training data\n",
    "my_review_words = None\n",
    "\n",
    "# TODO: Then call your classifier to label it\n",
    "vectorizer = None\n",
    "vectorizer = None\n",
    "my_review_features = None\n",
    "my_review_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(my_review_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
